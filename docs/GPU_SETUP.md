# üöÄ GPU Acceleration Guide for MOSAIC

This guide explains how to enable and configure GPU acceleration in MOSAIC for significantly faster video processing and search operations.

## üìä Performance Impact

GPU acceleration can provide **10-50x speedup** for:

- **Image Embedding Generation** (CLIP model): ~10-20x faster
- **Text Embedding Generation**: ~5-10x faster
- **FAISS Vector Search**: ~5-15x faster
- **Overall Video Processing**: ~3-10x faster end-to-end

### Example Timings (RTX 3080)

| Operation                            | CPU (i7-9700K) | GPU (RTX 3080) | Speedup |
| ------------------------------------ | -------------- | -------------- | ------- |
| Extract & embed 1000 frames          | ~120s          | ~8s            | **15x** |
| Search 10K frame database            | ~2.5s          | ~0.2s          | **12x** |
| Full video processing (10 min video) | ~8 min         | ~1 min         | **8x**  |

---

## ‚úÖ Prerequisites

### 1. Check GPU Compatibility

```powershell
# Check if you have an NVIDIA GPU
nvidia-smi
```

You need:

- **NVIDIA GPU** with CUDA support (Tesla, RTX, GTX series)
- **CUDA Compute Capability**: 3.5 or higher
- **VRAM**: Minimum 4GB, recommended 6GB+

### 2. Install CUDA Toolkit

Download and install from: https://developer.nvidia.com/cuda-downloads

**Recommended versions:**

- CUDA 11.8 or 12.1
- cuDNN 8.x

**Verify installation:**

```powershell
nvcc --version
```

---

## üîß Installation

### Option 1: Standard GPU Setup (Recommended)

1. **Install PyTorch with CUDA support:**

```bash
# For CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

2. **Install FAISS GPU (optional but recommended):**

```bash
# Uninstall CPU version
pip uninstall faiss-cpu -y

# Install GPU version
pip install faiss-gpu
```

3. **Verify GPU is detected:**

```bash
python scripts/gpu.py
```

Expected output:

```
üöÄ Using device: CUDA
   GPU: NVIDIA GeForce RTX 3080
   VRAM: 10.0 GB
Torch version: 2.x.x
CUDA available: True
CUDA version: 11.8
```

### Option 2: Docker with GPU Support

1. **Install NVIDIA Container Toolkit:**

```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

2. **Update docker-compose.yml:**

```yaml
services:
  mcp-server:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - DEVICE=cuda
```

3. **Run with GPU:**

```bash
docker-compose up
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

Add to your `.env` file:

```bash
# Device Configuration
DEVICE=auto  # auto, cuda, or cpu

# GPU-specific settings (optional)
CUDA_VISIBLE_DEVICES=0  # GPU ID to use (0, 1, 2...)
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # Reduce fragmentation
```

### Device Options

| Value  | Behavior                                               |
| ------ | ------------------------------------------------------ |
| `auto` | **Default** - Uses GPU if available, falls back to CPU |
| `cuda` | Forces GPU usage - fails if GPU not available          |
| `cpu`  | Forces CPU usage - useful for debugging                |

---

## üß™ Testing GPU Performance

### Quick Test

```bash
# Navigate to MCP server directory
cd mosaic-mcp

# Run GPU test
python -c "import torch; print('GPU Available:', torch.cuda.is_available())"
```

### Benchmark Video Processing

```bash
# Process a test video
python -c "
from src.video_processor import VideoProcessingPipeline
import time

processor = VideoProcessingPipeline()
start = time.time()
result = processor.process_video('test_video.mp4', 'test_id')
print(f'Processing time: {time.time() - start:.2f}s')
"
```

### Compare CPU vs GPU

```bash
# CPU benchmark
DEVICE=cpu python benchmark_script.py

# GPU benchmark
DEVICE=cuda python benchmark_script.py
```

---

## üìà Optimization Tips

### 1. Batch Size Tuning

Larger batch sizes utilize GPU better but require more VRAM:

```python
# In video_processor.py
BATCH_SIZE = 64  # Increase from 32 if you have 8GB+ VRAM
BATCH_SIZE = 128  # For 16GB+ VRAM
```

### 2. Mixed Precision (Advanced)

For newer GPUs (RTX 20xx+):

```python
# Add to model initialization
clip_model = SentenceTransformer('clip-ViT-B-32', device=device)
clip_model.half()  # Use FP16 instead of FP32 (2x faster, half VRAM)
```

### 3. FAISS GPU Index Types

```python
# Flat index (most accurate, slower)
index = faiss.IndexFlatL2(dimension)

# IVF index (faster, slight accuracy tradeoff)
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist=100)
```

### 4. Monitor GPU Usage

```bash
# Watch GPU utilization in real-time
watch -n 1 nvidia-smi
```

---

## üêõ Troubleshooting

### Issue: "CUDA out of memory"

**Solution:**

```bash
# Reduce batch size
BATCH_SIZE=16

# Or free GPU memory
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
```

### Issue: "CUDA not available" but GPU is present

**Solution:**

```bash
# Reinstall PyTorch with CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Issue: FAISS GPU installation fails

**Solution:**

```bash
# Use conda instead of pip
conda install -c pytorch faiss-gpu

# Or use CPU version (still fast for most use cases)
pip install faiss-cpu
```

### Issue: Slow GPU performance

**Checklist:**

- [ ] GPU drivers up to date
- [ ] CUDA version matches PyTorch
- [ ] Batch size is large enough (32+)
- [ ] Not running other GPU applications
- [ ] Sufficient VRAM available

---

## üîç Monitoring & Debugging

### Check Current Device

```python
from mosaic_mcp.src.video_processor import device
print(f"Current device: {device}")
```

### Log GPU Memory Usage

```python
import torch
if torch.cuda.is_available():
    print(f"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
```

### Enable Detailed Logging

```bash
# Set environment variable
export CUDA_LAUNCH_BLOCKING=1
export TORCH_SHOW_CPP_STACKTRACES=1
```

---

## üìö Additional Resources

- **CUDA Toolkit**: https://developer.nvidia.com/cuda-downloads
- **PyTorch GPU Setup**: https://pytorch.org/get-started/locally/
- **FAISS GPU Guide**: https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU
- **Sentence Transformers GPU**: https://www.sbert.net/docs/installation.html

---

## üí° Best Practices

1. ‚úÖ **Use `DEVICE=auto`** for automatic GPU detection
2. ‚úÖ **Monitor VRAM usage** with `nvidia-smi`
3. ‚úÖ **Batch process** frames for better GPU utilization
4. ‚úÖ **Keep GPU drivers updated**
5. ‚úÖ **Use GPU for production**, CPU for development/testing
6. ‚úÖ **Profile your workload** to identify bottlenecks

---

## üéØ Expected Performance Gains

| Hardware           | Video Processing Speed | Search Speed      |
| ------------------ | ---------------------- | ----------------- |
| CPU Only (8 cores) | 1x baseline            | 1x baseline       |
| RTX 3060 (12GB)    | **5-8x faster**        | **8-10x faster**  |
| RTX 3080 (10GB)    | **8-12x faster**       | **10-15x faster** |
| RTX 4090 (24GB)    | **15-20x faster**      | **15-25x faster** |
| A100 (40GB)        | **20-30x faster**      | **25-40x faster** |

---

**Happy accelerating! üöÄ**
